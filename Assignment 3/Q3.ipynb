{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4-Q3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPt5q27L5557"
      },
      "source": [
        "# MDA 2021\n",
        "## HW4-Q3\n",
        "-----------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-"
      },
      "source": [
        "## Setup\n",
        "--------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsj5WYpR9QId"
      },
      "source": [
        "Let's setup Spark on Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-qHai2252mI",
        "outputId": "f6a2acaa-b5a2-40fb-ad73-694bdb061097"
      },
      "source": [
        "!apt-get update --fix-missing\n",
        "!sudo apt update"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "85 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd2KwvWzEOnE",
        "outputId": "006b02ca-c42d-4649-9f4c-ddde71cd6e29"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.2)\n",
            "openjdk-8-jdk-headless is already the newest version (8u312-b07-0ubuntu1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 85 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "PdkxQI7N7Xzw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CJ71AKe91eh"
      },
      "source": [
        "Now we authenticate a Google Drive client to processing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K93ABEy9Zlo",
        "outputId": "9106a1d6-f303-4dd5-dc3c-843897674b3a"
      },
      "source": [
        "from google.colab import drive\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bK7ob3k4Yd1"
      },
      "source": [
        "## Check and extract data\n",
        "--------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT70OpRGzkWh",
        "outputId": "1649c168-88e9-48c4-fadd-56af0d12ed49"
      },
      "source": [
        "!ls '/content/drive/My Drive/Sample_Data.zip'"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/My Drive/Sample_Data.zip'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLoUDlZrzoAg",
        "outputId": "fb9be6d0-700b-4f65-bb11-720567060987"
      },
      "source": [
        "!unzip \"/content/drive/My Drive/Sample_Data.zip\" -d \"/content/drive/My Drive/Sampe_Data\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/My Drive/Sample_Data.zip\n",
            "replace /content/drive/My Drive/Sampe_Data/Sample_Traffic.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwtlO4_m_LbQ"
      },
      "source": [
        "\n",
        "the cells above, extract data which is in '/content/drive/My Drive/Sample_Data.zip' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRaF2A_j_nC7"
      },
      "source": [
        "## Initializing Spark and read data\n",
        "--------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDh957r_0snm"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf \n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,TimestampType\n",
        "from pyspark.sql.functions import col,current_timestamp,to_date,hour,dayofweek\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Spark_Processor\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc=spark.sparkContext\n",
        "\n",
        "schema = StructType([ \\\n",
        "        StructField(\"DEVICE_CODE\", IntegerType(), True), \n",
        "        StructField(\"SYSTEM_ID\",IntegerType(),True), \\\n",
        "        StructField(\"ORIGINE_CAR_KEY\",StringType(),True), \\\n",
        "        StructField(\"FINAL_CAR_KEY\",StringType(),True), \\\n",
        "        StructField(\"CHECK_STATUS_KEY\", IntegerType(), True), \\\n",
        "        StructField(\"COMPANY_ID\", StringType(), True), \\\n",
        "        StructField(\"PASS_DAY_TIME\", TimestampType(), True)\n",
        "    ])"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meztPeu2UJ_w"
      },
      "source": [
        "#part a\n",
        "\n",
        "import pandas as pd\n",
        "#first of all, we have to load the data\n",
        "data  = sc.textFile('/content/drive/My Drive/Sampe_Data/Sample_Traffic.csv')\n",
        "header = data.first()\n",
        "data = data.filter(lambda line: line != header)\n",
        "data = data.sample(False,0.001,101)\n",
        "#this function is capable of splitting columns of data\n",
        "#['DEVICE_CODE,SYSTEM_ID,ORIGINE_CAR_KEY,FINAL_CAR_KEY,CHECK_STATUS_KEY,COMPANY_ID,PASS_DAY_TIME']\n",
        "def parseLine(line):  \n",
        "  fields = line.split(',')\n",
        "  DEVICE_CODE = int(fields[0])\n",
        "  ORIGINE_CAR_KEY = int(fields[2])\n",
        "  PASS_DAY_TIME = pd.to_datetime(fields[6]).day\n",
        "  return (ORIGINE_CAR_KEY,PASS_DAY_TIME,DEVICE_CODE)\n",
        "\n",
        "traffic_rdd = data.map(parseLine)\n",
        "traffic_rdd.first()\n",
        "#print (200501,1,10477885) tuple\n",
        "\n",
        "traffic_rdd = traffic_rdd.map(lambda x:(tuple([x[0],x[1]]),x[2]))\n",
        "traffic_rdd_gb = sorted(traffic_rdd.groupByKey().mapValues(list).collect())"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_KE3O7Z--X7"
      },
      "source": [
        "traffic_rdd_gb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggmu2plOx5YK"
      },
      "source": [
        "#part 2\n",
        "path_baskets = []\n",
        "all_paths = []\n",
        "#erecting the baskets of paths\n",
        "for i in range(len(traffic_rdd_gb)):\n",
        "  path_baskets.append(traffic_rdd_gb[i][1])\n",
        "  #merging all the items\n",
        "  all_paths.extend(path_baskets[i])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "all_paths = list(np.unique(all_paths))\n",
        "adj_matrix = np.zeros((len(traffic_rdd_gb),len(all_paths)))\n",
        "for i in range(len(traffic_rdd_gb)):\n",
        "  for element in list(set(all_paths).intersection(traffic_rdd_gb[i][1])):\n",
        "    ind = all_paths.index(element)\n",
        "    adj_matrix[i][ind] = 1"
      ],
      "metadata": {
        "id": "7CJbB6TWFFmb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj_matrix[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsfqNG5-MTo-",
        "outputId": "8ed1a7db-4364-4e48-92a8-2ffe8f22b383"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0.])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(101)\n",
        "i = random.randint(0,len(all_paths))\n",
        "random_device_code_list = [0] * i + [1]*(len(all_paths) - i)\n",
        "random.shuffle(random_device_code_list)"
      ],
      "metadata": {
        "id": "tr1vwe8-NbFd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "cos_sim = np.zeros(len(traffic_rdd_gb))\n",
        "for i in range(len(traffic_rdd_gb)):\n",
        "  cos_sim[i] = dot(random_device_code_list, list(adj_matrix[i]))/(norm(random_device_code_list)*norm(list(adj_matrix[i])))\n",
        "cos_sim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju0VWBlcPr99",
        "outputId": "b023c652-b548-40ff-c363-e18b561b186d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.08333333, 0.        , 0.        , ..., 0.        , 0.08333333,\n",
              "       0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_5_idx = list(np.argsort(cos_sim)[-5:])\n",
        "for i in top_5_idx:\n",
        "  print(traffic_rdd_gb[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DYE-lkxR_4k",
        "outputId": "dd7d22d4-7240-48d8-b62b-dc9a73368aaf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((17149337, 1), [100700861, 900205])\n",
            "((86767658, 1), [900202, 100700868])\n",
            "((79163174, 1), [900139, 900216])\n",
            "((19113889, 1), [900216, 100700868])\n",
            "((23726171, 1), [631355, 100701129])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#part 3\n",
        "#LSH implementation with cos similarity\n",
        "import itertools\n",
        "import collections"
      ],
      "metadata": {
        "id": "OL9AW6SP9_ci"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lsh_euclidean(A, b, r, threshold):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    A: the (N,D) matrix of vectors to with which to find similar pairs\n",
        "    b: the number of bands\n",
        "    r: the number of rows per band\n",
        "    threshold: a float value [-1,1] determining the required cosine similarity threshold\n",
        "  Returns:\n",
        "    a set of pairs with requisite similarity.\n",
        "  \"\"\"\n",
        "  N, D = A.shape\n",
        "  n = b*r\n",
        "  \n",
        "  # Compute signature matrix\n",
        "  R = A@np.random.randn(D, n)\n",
        "  S = np.where(R>0, 1, 0)\n",
        "\n",
        "  # Break into bands\n",
        "  S = np.split(S, b, axis=1)\n",
        "\n",
        "  # column vector to convert binary vector to integer e.g. (1,0,1)->5\n",
        "  binary_column = 2**np.arange(r).reshape(-1,  1)\n",
        "\n",
        "  # convert each band into a single integer, \n",
        "  # convert band matrices to band columns\n",
        "  S = np.hstack([M@binary_column for M in S])\n",
        "\n",
        "  # Every value in the matrix represents a hash bucket assignment\n",
        "  # For every bucket in row i, add index i to that bucket\n",
        "  d = collections.defaultdict(set)\n",
        "  with np.nditer(S,flags=['multi_index']) as it:\n",
        "      for x in it:\n",
        "          d[int(x)].add(it.multi_index[0])\n",
        "\n",
        "  # For every bucket, find all pairs. These are the LSH pairs.\n",
        "  candidate_pairs = set()\n",
        "  for k,v in d.items():\n",
        "      if len(v) > 1:\n",
        "          for pair in itertools.combinations(v, 2):\n",
        "              candidate_pairs.add(tuple(sorted(pair)))\n",
        "\n",
        "  # Finally, perform the actually similarity computation\n",
        "  \n",
        "  lsh_pairs = set()\n",
        "  for (i,j) in candidate_pairs:\n",
        "    if i<441 : \n",
        "      if dot(random_device_code_list, list(A[i]))/(norm(random_device_code_list)*norm(list(A[i]))) > threshold:\n",
        "          lsh_pairs.add(i)\n",
        "    if j<441 : \n",
        "      if dot(random_device_code_list, list(A[j]))/(norm(random_device_code_list)*norm(list(A[j]))) > threshold:\n",
        "          lsh_pairs.add(j)\n",
        "  return lsh_pairs"
      ],
      "metadata": {
        "id": "SyAfrltLQhMP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lsh_euclidean(adj_matrix,8,2,0.1)"
      ],
      "metadata": {
        "id": "A52GnpkXRyF_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}